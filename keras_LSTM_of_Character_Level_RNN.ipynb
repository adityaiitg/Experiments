{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras LSTM of Character Level RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Learning from Andrej Karpathy"
      ],
      "metadata": {
        "id": "Sh07grBS-5-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "HGJ3enba-0FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading data\n",
        "data=open('hello.txt','r').read()\n",
        "#print(data)\n",
        "#print(set(data))\n",
        "chars=list(set(data))\n",
        "#print(chars)\n",
        "data_size,vocab_size = len(data),len(chars)\n",
        "print(\"data has %d characters, %f unique.\" % (data_size,vocab_size))\n",
        "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
        "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "# hyperparameter\n",
        "hidden_size = 100 # size of hidden layer of neurons\n",
        "seq_length = 25 # number of steps to unroll the RNN\n",
        "learning_rate = 1e-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "jT0jBfUd-4Yf",
        "outputId": "0fdeec7b-974a-4366-a25d-c7d0ecc50d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a913955dcd07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hello.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(set(data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hello.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all needed libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from time import time\n",
        "import aux_funcs as aux\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Activation, TimeDistributed\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import LambdaCallback, Callback\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.metrics import categorical_accuracy as accuracy\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "\n",
        "\n",
        "# Keras callbacks\n",
        "def test(txt_length):\n",
        "    txt = ''\n",
        "    seed = aux.encode([int(np.random.uniform(0, vocab_size))], vocab_size)\n",
        "    seed = np.array([seed])\n",
        "    init_state_h = np.zeros((1, batch_size, hidden_dim))\n",
        "    init_state_c = np.zeros((1, batch_size, hidden_dim))\n",
        "    for i in range(txt_length):\n",
        "        prob, init_state_h, init_state_c = pred_model.predict([seed, init_state_h, init_state_c])\n",
        "        pred = np.random.choice(range(vocab_size), p=prob[-1][0])\n",
        "        seed = np.expand_dims(aux.encode([pred], vocab_size), axis=0)\n",
        "        character = idx_to_char[pred]\n",
        "        txt += character\n",
        "    return txt\n",
        "\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = [-np.log(1.0 / vocab_size)]\n",
        "        self.smooth_loss = [-np.log(1.0 / vocab_size)]\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.smooth_loss.append(self.smooth_loss[-1] * 0.999 + logs.get('loss') * 0.001)\n",
        "        if batch % 100 == 0:\n",
        "            print(\"-----------------------------------\")\n",
        "            print(\"* Test *\")\n",
        "            print(logs)\n",
        "            print(\"-----------------------------------\")\n",
        "            print(test(600))\n",
        "            aux.plot(self.losses, self.smooth_loss, it, it_per_epoch, base_name=\"keras\")\n",
        "\n",
        "\n",
        "# load data\n",
        "data_name = 'quijote'\n",
        "input_file = data_name +'.txt'\n",
        "data, char_to_idx, idx_to_char, vocab_size = aux.load(input_file)\n",
        "print('data has %d characters, %d unique.' % (len(data), vocab_size))\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 1e-2\n",
        "seq_length = 100\n",
        "hidden_dim = 500\n",
        "batch_size = 1\n",
        "epochs = 5\n",
        "\n",
        "# instantiate generator\n",
        "it = 0\n",
        "reduce = 1/seq_length\n",
        "it_per_epoch = np.int(len(data) / (seq_length*reduce))\n",
        "p = (it % it_per_epoch) * seq_length\n",
        "data_feed = aux.keras_gen(data, seq_length, char_to_idx, vocab_size, p=p)\n",
        "\n",
        "# time counting starting here\n",
        "t0 = time()\n",
        "\n",
        "# callbacks\n",
        "history = LossHistory()\n",
        "\n",
        "# Model API\n",
        "inputs = Input(shape=(None, vocab_size))\n",
        "lstm_layer = LSTM(hidden_dim, return_sequences=True, return_state=True, stateful=False)\n",
        "lstm_output, _, _ = lstm_layer(inputs)\n",
        "dense_layer = Dense(vocab_size, activation='softmax')\n",
        "probabilities = dense_layer(lstm_output)\n",
        "model = Model(inputs=inputs, outputs=probabilities)\n",
        "\n",
        "\n",
        "state_input_h = Input(shape=(1, hidden_dim))\n",
        "state_input_c = Input(shape=(1, hidden_dim))\n",
        "lstm_pred_out, state_h, state_c = lstm_layer(inputs, initial_state=[state_input_h, state_input_c])\n",
        "pred_probabilities = dense_layer(lstm_pred_out)\n",
        "pred_model = Model(inputs=[inputs, state_input_h, state_input_c],\n",
        "                   outputs=[pred_probabilities, state_h, state_c])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate))\n",
        "print(model.summary())\n",
        "epochs_log = model.fit_generator(data_feed, steps_per_epoch=it_per_epoch, shuffle=False,\n",
        "                                 epochs=epochs, callbacks=[history], verbose=0)\n",
        "\n",
        "# final time\n",
        "print(\"Total time was: \", time() - t0)\n",
        "\n",
        "# loss history plot\n",
        "it = it_per_epoch * epochs\n",
        "aux.plot(history.losses, history.smooth_loss, it, it_per_epoch, base_name=\"keras\")"
      ],
      "metadata": {
        "id": "O7q4zk3a79o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(None, vocab_size))\n",
        "lstm_layer = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
        "lstm_output, _, _ = lstm_layer(inputs)\n",
        "dense_layer = Dense(vocab_size, activation='softmax')\n",
        "probabilities = dense_layer(lstm_output)\n",
        "model = Model(inputs=inputs, outputs=probabilities)"
      ],
      "metadata": {
        "id": "v5lsDrNE3U1H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}